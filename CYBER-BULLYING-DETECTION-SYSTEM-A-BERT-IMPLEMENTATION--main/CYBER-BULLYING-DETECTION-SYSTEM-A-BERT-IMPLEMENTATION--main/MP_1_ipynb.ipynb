{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FTuTnsT4OHk"
      },
      "source": [
        "## Custom functions definition:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5ijsQnQ4OHk"
      },
      "outputs": [],
      "source": [
        "def conf_matrix(y, y_pred, title, labels):\n",
        "    fig, ax =plt.subplots(figsize=(7.5,7.5))\n",
        "    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Purples\", fmt='g', cbar=False, annot_kws={\"size\":30})\n",
        "    plt.title(title, fontsize=25)\n",
        "    ax.xaxis.set_ticklabels(labels, fontsize=16) \n",
        "    ax.yaxis.set_ticklabels(labels, fontsize=14.5)\n",
        "    ax.set_ylabel('Test', fontsize=25)\n",
        "    ax.set_xlabel('Predicted', fontsize=25)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWkg5i9_4OHl"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0GOzHmK5QsH",
        "outputId": "8014d560-2b9a-4b48-bc8a-bda325512fbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 112 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 122 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 163 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 174 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 15.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=78fe0f1fa64e689109a0adbbb0b3704a85783d4d01caf2ea4de54b8d428d61bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 54.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n"
          ]
        }
      ],
      "source": [
        "#Libraries for general purpose\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Text cleaning\n",
        "import re, string\n",
        "!pip install emoji\n",
        "import emoji\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#Data preprocessing\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "#Naive Bayes\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        "#PyTorch LSTM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#Tokenization for LSTM\n",
        "from collections import Counter\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "#Transformers library for BERT\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#Seed for reproducibility\n",
        "import random\n",
        "\n",
        "seed_value=42\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "import time\n",
        "\n",
        "#set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.despine()\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "plt.rc(\"figure\", autolayout=True)\n",
        "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n",
        "%matplotlib inline\n",
        "!pip install gensim\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN8oRxJVuIuK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGfeyYaH4OHm"
      },
      "source": [
        "## Data Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCVJ1XJi5QsI"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"cyberbullying_tweets.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NPhe4rM35QsJ",
        "outputId": "e877413f-3edf-4f5e-e387-ee9388ceaebc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          tweet_text cyberbullying_type\n",
              "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
              "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
              "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
              "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
              "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5e465a89-9516-4bdd-b8d1-b2c5b2a75f3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was cra...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e465a89-9516-4bdd-b8d1-b2c5b2a75f3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5e465a89-9516-4bdd-b8d1-b2c5b2a75f3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5e465a89-9516-4bdd-b8d1-b2c5b2a75f3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji7kd2cd4OHn",
        "outputId": "b1baeeae-f46e-4152-ef32-74b9faf74def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 47692 entries, 0 to 47691\n",
            "Data columns (total 2 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   tweet_text          47692 non-null  object\n",
            " 1   cyberbullying_type  47692 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 745.3+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwUN352x5QsK"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns={'tweet_text': 'text', 'cyberbullying_type': 'sentiment'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c82luV2N5QsN",
        "outputId": "80cae7af-49fa-4e84-96fa-f1297889a7d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "religion               7998\n",
              "age                    7992\n",
              "gender                 7973\n",
              "ethnicity              7961\n",
              "not_cyberbullying      7945\n",
              "other_cyberbullying    7823\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.sentiment.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGmYrF4d5QsP"
      },
      "source": [
        "# Tweets text deep cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5E1UwMc5QsQ"
      },
      "outputs": [],
      "source": [
        "##CUSTOM DEFINED FUNCTIONS TO CLEAN THE TWEETS\n",
        "\n",
        "#Clean emojis from text\n",
        "def strip_emoji(text):\n",
        "    return re.sub(emoji.get_emoji_regexp(), r\"\", text) #remove emoji\n",
        "\n",
        "#Remove punctuations, links, stopwords, mentions and \\r\\n new line characters\n",
        "def strip_all_entities(text): \n",
        "    text = text.replace('\\r', '').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n",
        "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n",
        "    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
        "    banned_list= string.punctuation\n",
        "    table = str.maketrans('', '', banned_list)\n",
        "    text = text.translate(table)\n",
        "    text = [word for word in text.split() if word not in stop_words]\n",
        "    text = ' '.join(text)\n",
        "    text =' '.join(word for word in text.split() if len(word) < 14) # remove words longer than 14 characters\n",
        "    return text\n",
        "\n",
        "#remove contractions\n",
        "def decontract(text):\n",
        "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    return text\n",
        "\n",
        "#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the \"#\" symbol\n",
        "def clean_hashtags(tweet):\n",
        "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n",
        "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n",
        "    return new_tweet2\n",
        "\n",
        "#Filter special characters such as \"&\" and \"$\" present in some words\n",
        "def filter_chars(a):\n",
        "    sent = []\n",
        "    for word in a.split(' '):\n",
        "        if ('$' in word) | ('&' in word):\n",
        "            sent.append('')\n",
        "        else:\n",
        "            sent.append(word)\n",
        "    return ' '.join(sent)\n",
        "\n",
        "#Remove multiple sequential spaces\n",
        "def remove_mult_spaces(text):\n",
        "    return re.sub(\"\\s\\s+\" , \" \", text)\n",
        "\n",
        "#Stemming\n",
        "def stemmer(text):\n",
        "    tokenized = nltk.word_tokenize(text)\n",
        "    ps = PorterStemmer()\n",
        "    return ' '.join([ps.stem(words) for words in tokenized])\n",
        "\n",
        "#Lemmatization \n",
        "#NOTE:Stemming seems to work better for this dataset\n",
        "def lemmatize(text):\n",
        "    tokenized = nltk.word_tokenize(text)\n",
        "    lm = WordNetLemmatizer()\n",
        "    return ' '.join([lm.lemmatize(words) for words in tokenized])\n",
        "\n",
        "#Then we apply all the defined functions in the following order\n",
        "def deep_clean(text):\n",
        "    text = strip_emoji(text)\n",
        "    text = decontract(text)\n",
        "    text = strip_all_entities(text)\n",
        "    text = clean_hashtags(text)\n",
        "    text = filter_chars(text)\n",
        "    text = remove_mult_spaces(text)\n",
        "    text = stemmer(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx-xXilI5QsT"
      },
      "outputs": [],
      "source": [
        "texts_new = []\n",
        "for t in df.text:\n",
        "    texts_new.append(deep_clean(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah-xjjhv5QsT"
      },
      "outputs": [],
      "source": [
        "df['text_clean'] = texts_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4qEoAkeO4OHq",
        "outputId": "4a708caa-c643-47cb-a9ee-488450ecef2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text          sentiment  \\\n",
              "0  In other words #katandandre, your food was cra...  not_cyberbullying   \n",
              "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
              "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
              "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
              "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
              "\n",
              "                                          text_clean  \n",
              "0                 word katandandr food crapilici mkr  \n",
              "1  aussietv white mkr theblock today sunris studi...  \n",
              "2                     classi whore red velvet cupcak  \n",
              "3  meh p thank head concern anoth angri dude twitter  \n",
              "4  isi account pretend kurdish account like islam...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc9ab77d-54b7-4e5b-8ba6-21d172733e34\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was cra...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>word katandandr food crapilici mkr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>aussietv white mkr theblock today sunris studi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>classi whore red velvet cupcak</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>meh p thank head concern anoth angri dude twitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>isi account pretend kurdish account like islam...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc9ab77d-54b7-4e5b-8ba6-21d172733e34')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc9ab77d-54b7-4e5b-8ba6-21d172733e34 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc9ab77d-54b7-4e5b-8ba6-21d172733e34');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhgyrUQrdICU",
        "outputId": "88004a13-9ff6-4870-c27c-2dc8d8e11b05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47692, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5DHDI7QdCfu",
        "outputId": "c89f0edf-e60d-40d3-ce84-5e5d9c236ba0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3094"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df[\"text_clean\"].duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLqSkw924OHr"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(\"text_clean\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdCDSt1PdpXG",
        "outputId": "3f2de997-b773-4db8-c993-1159273fbffc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(44598, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Pagq6JYQhOHL",
        "outputId": "6d79c38e-c4d5-4f78-e32a-72c7c2aacac4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text          sentiment  \\\n",
              "0      In other words #katandandre, your food was cra...  not_cyberbullying   \n",
              "1      Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
              "2      @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
              "3      @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
              "4      @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
              "...                                                  ...                ...   \n",
              "47687  Black ppl aren't expected to do anything, depe...          ethnicity   \n",
              "47688  Turner did not withhold his disappointment. Tu...          ethnicity   \n",
              "47689  I swear to God. This dumb nigger bitch. I have...          ethnicity   \n",
              "47690  Yea fuck you RT @therealexel: IF YOURE A NIGGE...          ethnicity   \n",
              "47691  Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...          ethnicity   \n",
              "\n",
              "                                              text_clean  \n",
              "0                     word katandandr food crapilici mkr  \n",
              "1      aussietv white mkr theblock today sunris studi...  \n",
              "2                         classi whore red velvet cupcak  \n",
              "3      meh p thank head concern anoth angri dude twitter  \n",
              "4      isi account pretend kurdish account like islam...  \n",
              "...                                                  ...  \n",
              "47687  black ppl expect anyth depend anyth yet free p...  \n",
              "47688  turner withhold turner call court abomin concl...  \n",
              "47689  swear god dumb nigger bitch got bleach hair re...  \n",
              "47690  yea fuck rt your nigger fuck unfollow fuck dum...  \n",
              "47691  bro u got ta chill rt dog fuck kp dumb nigger ...  \n",
              "\n",
              "[44598 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f93da69-53cd-4136-802f-aff8d76509c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was cra...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>word katandandr food crapilici mkr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>aussietv white mkr theblock today sunris studi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>classi whore red velvet cupcak</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>meh p thank head concern anoth angri dude twitter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>isi account pretend kurdish account like islam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47687</th>\n",
              "      <td>Black ppl aren't expected to do anything, depe...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>black ppl expect anyth depend anyth yet free p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47688</th>\n",
              "      <td>Turner did not withhold his disappointment. Tu...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>turner withhold turner call court abomin concl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47689</th>\n",
              "      <td>I swear to God. This dumb nigger bitch. I have...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>swear god dumb nigger bitch got bleach hair re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47690</th>\n",
              "      <td>Yea fuck you RT @therealexel: IF YOURE A NIGGE...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>yea fuck rt your nigger fuck unfollow fuck dum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47691</th>\n",
              "      <td>Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>bro u got ta chill rt dog fuck kp dumb nigger ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>44598 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f93da69-53cd-4136-802f-aff8d76509c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f93da69-53cd-4136-802f-aff8d76509c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f93da69-53cd-4136-802f-aff8d76509c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxQj0MBZ4OHs",
        "outputId": "ebf93d5c-e59a-4ad5-82ce-590869439074"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "religion               7946\n",
              "age                    7884\n",
              "ethnicity              7744\n",
              "not_cyberbullying      7637\n",
              "gender                 7607\n",
              "other_cyberbullying    5780\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "df.sentiment.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I33_dhYy4OHs"
      },
      "outputs": [],
      "source": [
        "df = df[df[\"sentiment\"]!=\"other_cyberbullying\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRw4cbsj4OHs"
      },
      "outputs": [],
      "source": [
        "sentiments = [\"religion\",\"age\",\"ethnicity\",\"gender\",\"not bullying\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLwRaYcQ4OHs"
      },
      "source": [
        "# Tweets length analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQsHVXTA5QsV"
      },
      "outputs": [],
      "source": [
        "text_len = []\n",
        "for text in df.text_clean:\n",
        "    tweet_len = len(text.split())\n",
        "    text_len.append(tweet_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op665CKx5QsV"
      },
      "outputs": [],
      "source": [
        "df['text_len'] = text_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "HZcu8uGh5QsW",
        "outputId": "ae2bbab6-5d42-4c9f-e24c-e03c74336197"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nplt.figure(figsize=(7,5))\\nax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\\nplt.title('Count of tweets with less than 10 words', fontsize=20)\\nplt.yticks([])\\nax.bar_label(ax.containers[0])\\nplt.ylabel('count')\\nplt.xlabel('')\\nplt.show()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "'''\n",
        "plt.figure(figsize=(7,5))\n",
        "ax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\n",
        "plt.title('Count of tweets with less than 10 words', fontsize=20)\n",
        "plt.yticks([])\n",
        "ax.bar_label(ax.containers[0])\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('')\n",
        "plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rEvtAXE5QsX"
      },
      "outputs": [],
      "source": [
        "df = df[df['text_len'] > 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2Dv2aZRv5QsY",
        "outputId": "406cf6e1-a331-4fb4-dee9-074c619516ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text          sentiment  \\\n",
              "44035  You so black and white trying to live like a n...          ethnicity   \n",
              "45165  @hermdiggz: “@tayyoung_: FUCK OBAMA, dumb ass ...          ethnicity   \n",
              "33724  ... I don't feel guilty for killing him, I jus...                age   \n",
              "1317   @EurekAlertAAAS: Researchers push to import to...  not_cyberbullying   \n",
              "47037  @Purely_Ambition: Sooo mad. RT @TracePeterson ...          ethnicity   \n",
              "...                                                  ...                ...   \n",
              "10274  @holliebakerlutz What does that 23% figure rep...             gender   \n",
              "5229                      are bully sticks for dogs safe  not_cyberbullying   \n",
              "10314  RT @Mr_LayedBak: I'm not sexist... but women r...             gender   \n",
              "5237                      *hands you a bag of trail mix*  not_cyberbullying   \n",
              "8275   @feministlah @GreenWeiner Picture was gender n...             gender   \n",
              "\n",
              "                                              text_clean  text_len  \n",
              "44035  black white tri live like nigger pahahahaha co...       187  \n",
              "45165  fuck obama dumb ass nigger bitch ltthi whore s...       162  \n",
              "33724  feel guilti kill feel guilti enjoy torment sin...       137  \n",
              "1317   research push import top antibulli program us ...       137  \n",
              "47037  sooo mad rt fuck obama dumb nigger go switzerl...       125  \n",
              "...                                                  ...       ...  \n",
              "10274                              23 figur repres deriv         4  \n",
              "5229                                bulli stick dog safe         4  \n",
              "10314                                rt sexist women ref         4  \n",
              "5237                                  hand bag trail mix         4  \n",
              "8275                        pictur gender neutral butsur         4  \n",
              "\n",
              "[37122 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4eb494c9-c771-43a1-8458-fd066d6e4a17\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>text_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>44035</th>\n",
              "      <td>You so black and white trying to live like a n...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>black white tri live like nigger pahahahaha co...</td>\n",
              "      <td>187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45165</th>\n",
              "      <td>@hermdiggz: “@tayyoung_: FUCK OBAMA, dumb ass ...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>fuck obama dumb ass nigger bitch ltthi whore s...</td>\n",
              "      <td>162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33724</th>\n",
              "      <td>... I don't feel guilty for killing him, I jus...</td>\n",
              "      <td>age</td>\n",
              "      <td>feel guilti kill feel guilti enjoy torment sin...</td>\n",
              "      <td>137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>@EurekAlertAAAS: Researchers push to import to...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>research push import top antibulli program us ...</td>\n",
              "      <td>137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47037</th>\n",
              "      <td>@Purely_Ambition: Sooo mad. RT @TracePeterson ...</td>\n",
              "      <td>ethnicity</td>\n",
              "      <td>sooo mad rt fuck obama dumb nigger go switzerl...</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10274</th>\n",
              "      <td>@holliebakerlutz What does that 23% figure rep...</td>\n",
              "      <td>gender</td>\n",
              "      <td>23 figur repres deriv</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5229</th>\n",
              "      <td>are bully sticks for dogs safe</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>bulli stick dog safe</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10314</th>\n",
              "      <td>RT @Mr_LayedBak: I'm not sexist... but women r...</td>\n",
              "      <td>gender</td>\n",
              "      <td>rt sexist women ref</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5237</th>\n",
              "      <td>*hands you a bag of trail mix*</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>hand bag trail mix</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8275</th>\n",
              "      <td>@feministlah @GreenWeiner Picture was gender n...</td>\n",
              "      <td>gender</td>\n",
              "      <td>pictur gender neutral butsur</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>37122 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4eb494c9-c771-43a1-8458-fd066d6e4a17')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4eb494c9-c771-43a1-8458-fd066d6e4a17 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4eb494c9-c771-43a1-8458-fd066d6e4a17');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "df.sort_values(by=['text_len'], ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Zt9YFv935QsY",
        "outputId": "7819e61a-527b-451b-8ae2-c610cb4bf514"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nplt.figure(figsize=(16,5))\\nax = sns.countplot(x='text_len', data=df[(df['text_len']<=1000) & (df['text_len']>10)], palette='Blues_r')\\nplt.title('Count of tweets with high number of words', fontsize=25)\\nplt.yticks([])\\nax.bar_label(ax.containers[0])\\nplt.ylabel('count')\\nplt.xlabel('')\\nplt.show()\\n\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.countplot(x='text_len', data=df[(df['text_len']<=1000) & (df['text_len']>10)], palette='Blues_r')\n",
        "plt.title('Count of tweets with high number of words', fontsize=25)\n",
        "plt.yticks([])\n",
        "ax.bar_label(ax.containers[0])\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('')\n",
        "plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teb7a02F5QsY"
      },
      "outputs": [],
      "source": [
        "df = df[df['text_len'] < 100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyQak-0QI9Mk",
        "outputId": "08b52b32-2ab2-4b75-da16-325e9c7f39bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_len = np.max(df['text_len'])\n",
        "max_len "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "NZD-8GJd5Qsa",
        "outputId": "ddf3f996-f658-4498-df1f-1b6e3911c1e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-92bce8e8-7207-4f62-b526-c4563e9f3e0e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>text_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4846</th>\n",
              "      <td>@andrea_gcav: @viviaanajim recuerdas como noso...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>recuerda como nosotra tambin eramo victima del...</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21241</th>\n",
              "      <td>And yet God was able to meet their needs using...</td>\n",
              "      <td>religion</td>\n",
              "      <td>yet god abl meet need use radic everyday gener...</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20862</th>\n",
              "      <td>Abee ,,,atuee ,,aloo kabheee Desh kee Flag koo...</td>\n",
              "      <td>religion</td>\n",
              "      <td>abe atue aloo kabhee desh kee flag koo bhee sa...</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18019</th>\n",
              "      <td>U shud understand dat nobody wants 2 stay wit ...</td>\n",
              "      <td>religion</td>\n",
              "      <td>u shud understand dat nobodi want 2 stay wit h...</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19900</th>\n",
              "      <td>1st u hav need to knw abut kashmiri hindu , te...</td>\n",
              "      <td>religion</td>\n",
              "      <td>1st u hav need knw abut kashmiri hindu tell sm...</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7759</th>\n",
              "      <td>Don't think I'm watching a 4th round of</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>think watch 4th round</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17740</th>\n",
              "      <td>RT @emanin: Bloody Islamic Bastards https://t....</td>\n",
              "      <td>religion</td>\n",
              "      <td>rt bloodi islam bastard</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13065</th>\n",
              "      <td>Looks like shit #cuntandandre....not surprising</td>\n",
              "      <td>gender</td>\n",
              "      <td>look like shit surpris</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>705</th>\n",
              "      <td>Phahahahaha The classics are the best #TheCobs...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>phahahahaha classic best thecobsyshow</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3535</th>\n",
              "      <td>@LeoKikiLady89 I respect your use of Sprinkles...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>respect use sprinkl winner</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>21575 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92bce8e8-7207-4f62-b526-c4563e9f3e0e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-92bce8e8-7207-4f62-b526-c4563e9f3e0e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-92bce8e8-7207-4f62-b526-c4563e9f3e0e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    text          sentiment  \\\n",
              "4846   @andrea_gcav: @viviaanajim recuerdas como noso...  not_cyberbullying   \n",
              "21241  And yet God was able to meet their needs using...           religion   \n",
              "20862  Abee ,,,atuee ,,aloo kabheee Desh kee Flag koo...           religion   \n",
              "18019  U shud understand dat nobody wants 2 stay wit ...           religion   \n",
              "19900  1st u hav need to knw abut kashmiri hindu , te...           religion   \n",
              "...                                                  ...                ...   \n",
              "7759             Don't think I'm watching a 4th round of  not_cyberbullying   \n",
              "17740  RT @emanin: Bloody Islamic Bastards https://t....           religion   \n",
              "13065    Looks like shit #cuntandandre....not surprising             gender   \n",
              "705    Phahahahaha The classics are the best #TheCobs...  not_cyberbullying   \n",
              "3535   @LeoKikiLady89 I respect your use of Sprinkles...  not_cyberbullying   \n",
              "\n",
              "                                              text_clean  text_len  \n",
              "4846   recuerda como nosotra tambin eramo victima del...        79  \n",
              "21241  yet god abl meet need use radic everyday gener...        43  \n",
              "20862  abe atue aloo kabhee desh kee flag koo bhee sa...        42  \n",
              "18019  u shud understand dat nobodi want 2 stay wit h...        42  \n",
              "19900  1st u hav need knw abut kashmiri hindu tell sm...        42  \n",
              "...                                                  ...       ...  \n",
              "7759                               think watch 4th round         4  \n",
              "17740                            rt bloodi islam bastard         4  \n",
              "13065                             look like shit surpris         4  \n",
              "705                phahahahaha classic best thecobsyshow         4  \n",
              "3535                          respect use sprinkl winner         4  \n",
              "\n",
              "[21575 rows x 4 columns]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sort_values(by=[\"text_len\"], ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx0IFdyO4OHv"
      },
      "source": [
        "## Sentiment column encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIS_nyXBG416"
      },
      "outputs": [],
      "source": [
        "df['sentiment'] = df['sentiment'].replace({'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELMGaiE-F7yn"
      },
      "source": [
        "## Train - Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF01CgBtBONZ"
      },
      "outputs": [],
      "source": [
        "X = df['text_clean']\n",
        "y = df['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvQexohPGAZf"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0d0lSX0GbNS"
      },
      "source": [
        "##**Train - Validation split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KyTh6H5GbR4"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-mC3qyuBONc",
        "outputId": "12e52ece-49cd-4b88-8602-84c129ba430c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   0, 5683],\n",
              "       [   3, 5264],\n",
              "       [   4, 4587]])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(unique, counts) = np.unique(y_train, return_counts=True)\n",
        "np.asarray((unique, counts)).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Geqa3e3zGUNL"
      },
      "source": [
        "# Oversampling of training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN5OCli8BONe"
      },
      "outputs": [],
      "source": [
        "ros = RandomOverSampler()\n",
        "X_train, y_train = ros.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train).reshape(-1, 1));\n",
        "train_os = pd.DataFrame(list(zip([x[0] for x in X_train], y_train)), columns = ['text_clean', 'sentiment']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAwnlrGoBONe"
      },
      "outputs": [],
      "source": [
        "X_train = train_os['text_clean'].values\n",
        "y_train = train_os['sentiment'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNopbN4gBONe",
        "outputId": "efa949bb-c319-4805-e9b7-0b1f31a88ad9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   0, 5683],\n",
              "       [   3, 5683],\n",
              "       [   4, 5683]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(unique, counts) = np.unique(y_train, return_counts=True)\n",
        "np.asarray((unique, counts)).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2mww5GA4OHz"
      },
      "source": [
        "# PyTorch Bi-LSTM RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZpta6PbBONf"
      },
      "source": [
        "# Data preprocessing for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAhPX58mpM0i"
      },
      "outputs": [],
      "source": [
        "def Tokenize(column, seq_len):\n",
        "    ##Create vocabulary of words from column\n",
        "    corpus = [word for text in column for word in text.split()]\n",
        "    count_words = Counter(corpus)\n",
        "    sorted_words = count_words.most_common()\n",
        "    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
        "\n",
        "    ##Tokenize the columns text using the vocabulary\n",
        "    text_int = []\n",
        "    for text in column:\n",
        "        r = [vocab_to_int[word] for word in text.split()]\n",
        "        text_int.append(r)\n",
        "    ##Add padding to tokens\n",
        "    features = np.zeros((len(text_int), seq_len), dtype = int)\n",
        "    for i, review in enumerate(text_int):\n",
        "        if len(review) <= seq_len:\n",
        "            zeros = list(np.zeros(seq_len - len(review)))\n",
        "            new = zeros + review\n",
        "        else:\n",
        "            new = review[: seq_len]\n",
        "        features[i, :] = np.array(new)\n",
        "\n",
        "    return sorted_words, features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Huovl-tkq9pr"
      },
      "outputs": [],
      "source": [
        "vocabulary, tokenized_column = Tokenize(df[\"text_clean\"], max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4GTRXEE04OH0",
        "outputId": "7175d013-ccbb-4c52-e664-63c46c43ba61"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'love best respons hotcak manag film noncommitt meh adolesc mkr'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"text_clean\"].iloc[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akCQyj-14OH0",
        "outputId": "88de97f0-5fcd-4f0c-b105-42c1a08d40a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,    58,   281,   340,\n",
              "        5930,  1309,  1144, 10164,  3732, 10165,    11])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_column[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14BsBTu64OH0"
      },
      "outputs": [],
      "source": [
        "keys = []\n",
        "values = []\n",
        "for key, value in vocabulary[:20]:\n",
        "    keys.append(key)\n",
        "    values.append(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "id": "77AveW484OH1",
        "outputId": "ae8fb9eb-9d2e-4b13-b973-c9d1b737d225"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-73aa2d8767b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mako'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Top 20 most common words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Words count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'AxesSubplot' object has no attribute 'bar_label'"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#from axesSubplot isport bar_label\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = sns.barplot(keys, values, palette='mako')\n",
        "plt.title('Top 20 most common words', size=25)\n",
        "ax.bar_label(ax.containers[0])\n",
        "plt.ylabel(\"Words count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTbzqr0u4OH1"
      },
      "source": [
        "# Word Embedding by Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8Z4L0Eg4OH1"
      },
      "outputs": [],
      "source": [
        "Word2vec_train_data = list(map(lambda x: x.split(), X_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-Eq2axK4OH1"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLKrxBZW4OH2",
        "outputId": "7b32eff5-3c75-45d4-94aa-e2b154c0db2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "\n",
        "word2vec_model = Word2Vec(Word2vec_train_data, size=EMBEDDING_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-I8oY564OH2",
        "outputId": "f5f7fd7d-a9fb-4b43-bb84-5793c97f960b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 24116\n"
          ]
        }
      ],
      "source": [
        "print(f\"Vocabulary size: {len(vocabulary) + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIx4VmUC4OH2"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = len(vocabulary) + 1 #+1 for the padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7-_vv3l4OH2",
        "outputId": "a236e446-3332-4a03-b2c9-c1c0d7d60c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Matrix Shape: (24116, 200)\n"
          ]
        }
      ],
      "source": [
        "#define empty embedding matrix\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
        "    \n",
        "#fill the embedding matrix with the pre trained values from word2vec\n",
        "#    corresponding to word (string), token (number associated to the word)\n",
        "for word, token in vocabulary:\n",
        "    if word2vec_model.wv.__contains__(word):\n",
        "        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n",
        "\n",
        "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWSPpkgh4OH2"
      },
      "source": [
        "## Train - Validation - Test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0otPBTtQ4OH2"
      },
      "source": [
        "Now we will use the tokenized sentences to create a training, validation and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPzbx9jFBONg"
      },
      "outputs": [],
      "source": [
        "X = tokenized_column\n",
        "y = df['sentiment'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQJ-XSMzBONg"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeca_zUVBONg"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=seed_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcx-DbFQ4OH3"
      },
      "source": [
        "We can check the balance of the target classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX77tlMuBONg",
        "outputId": "a02568ae-2f45-48f8-fb3b-f36be2075dcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   0, 5683],\n",
              "       [   3, 5264],\n",
              "       [   4, 4587]])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(unique, counts) = np.unique(y_train, return_counts=True)\n",
        "np.asarray((unique, counts)).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-bn4jLJ4OH3"
      },
      "source": [
        "And then apply random oversampling on the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFRMuSdrBONh"
      },
      "outputs": [],
      "source": [
        "ros = RandomOverSampler()\n",
        "X_train_os, y_train_os = ros.fit_resample(np.array(X_train),np.array(y_train));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQl1UY2BUt-m",
        "outputId": "e409ed2f-af0a-46d6-9377-6f908f565bac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   0, 5683],\n",
              "       [   3, 5683],\n",
              "       [   4, 5683]])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(unique, counts) = np.unique(y_train_os, return_counts=True)\n",
        "np.asarray((unique, counts)).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD2NQRAU4OH4"
      },
      "source": [
        "## PyTorch datasets and dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHGFwoZ64OH4"
      },
      "source": [
        "The three sets will be transformed to tensor datasets and dataloaders so we can extract the data in batches for the LSTM training, validation and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-e9FHZp5Qsc"
      },
      "outputs": [],
      "source": [
        "train_data = TensorDataset(torch.from_numpy(X_train_os), torch.from_numpy(y_train_os))\n",
        "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfzfdlUFX8al"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2TGQjB4X7UQ"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True) \n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjv7lTKd4OH4"
      },
      "source": [
        "# PyTorch LSTM modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EtgvVWY4OH4"
      },
      "source": [
        "Finally we can start the LSTM modeling. We start by setting some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfLClXUs5Qsc"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 5 #We are dealing with a multiclass classification of 5 classes\n",
        "HIDDEN_DIM = 100 #number of neurons of the internal state (internal neural network in the LSTM)\n",
        "LSTM_LAYERS = 1 #Number of stacked LSTM layers\n",
        "\n",
        "LR = 3e-4 #Learning rate\n",
        "DROPOUT = 0.5 #LSTM Dropout\n",
        "BIDIRECTIONAL = True #Boolean value to choose if to use a bidirectional LSTM or not\n",
        "EPOCHS = 5 #Number of training epoch\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InaVLcNV5Qsc"
      },
      "outputs": [],
      "source": [
        "class BiLSTM_Sentiment_Classifier(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, lstm_layers, bidirectional,batch_size, dropout):\n",
        "        super(BiLSTM_Sentiment_Classifier,self).__init__()\n",
        "        \n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers=lstm_layers,\n",
        "                            dropout=dropout,\n",
        "                            bidirectional=bidirectional,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim*self.num_directions, num_classes)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        self.batch_size = x.size(0)\n",
        "        ##EMBEDDING LAYER\n",
        "        embedded = self.embedding(x)\n",
        "        #LSTM LAYERS\n",
        "        out, hidden = self.lstm(embedded, hidden)\n",
        "        #Extract only the hidden state from the last LSTM cell\n",
        "        out = out[:,-1,:]\n",
        "        #FULLY CONNECTED LAYERS\n",
        "        out = self.fc(out)\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        #Initialization of the LSTM hidden and cell states\n",
        "        h0 = torch.zeros((self.lstm_layers*self.num_directions, batch_size, self.hidden_dim)).detach().to(DEVICE)\n",
        "        c0 = torch.zeros((self.lstm_layers*self.num_directions, batch_size, self.hidden_dim)).detach().to(DEVICE)\n",
        "        hidden = (h0, c0)\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3Zs0lfW5Qsd",
        "outputId": "2f88dbc1-48de-4910-c058-e2552e50f149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BiLSTM_Sentiment_Classifier(\n",
            "  (embedding): Embedding(24116, 200)\n",
            "  (lstm): LSTM(200, 100, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=200, out_features=5, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = BiLSTM_Sentiment_Classifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM,NUM_CLASSES, LSTM_LAYERS,BIDIRECTIONAL, BATCH_SIZE, DROPOUT)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "#Initialize embedding with the previously defined embedding matrix\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "#Allow the embedding matrix to be fined tuned to better adapt to out dataset and get higher accuracy\n",
        "model.embedding.weight.requires_grad=True\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftkcRBFCY-Zp"
      },
      "outputs": [],
      "source": [
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay = 5e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qQDp2AP4OH5"
      },
      "source": [
        "# LSTM Training loop "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXptvKsU4OH6"
      },
      "source": [
        "Now we will define a custom training loop, where we include an early stopping functionality, and save only the best models in terms of validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rK3YPVT5Qsd",
        "outputId": "e0884c81-58e0-4624-fd15-c9cbc862e2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1:Validation accuracy increased (0.000000 --> 90.860849).  Saving model ...\n",
            "\tTrain_loss : 0.7221 Val_loss : 0.2684\n",
            "\tTrain_acc : 67.387% Val_acc : 90.861%\n",
            "Epoch 2:Validation accuracy increased (90.860849 --> 91.450472).  Saving model ...\n",
            "\tTrain_loss : 0.2165 Val_loss : 0.2320\n",
            "\tTrain_acc : 92.270% Val_acc : 91.450%\n",
            "Epoch 3:Validation accuracy increased (91.450472 --> 92.570755).  Saving model ...\n",
            "\tTrain_loss : 0.1405 Val_loss : 0.2276\n",
            "\tTrain_acc : 95.125% Val_acc : 92.571%\n",
            "Epoch 4:Validation accuracy did not increase\n",
            "\tTrain_loss : 0.0916 Val_loss : 0.2361\n",
            "\tTrain_acc : 96.898% Val_acc : 92.453%\n",
            "Epoch 5:Validation accuracy increased (92.570755 --> 92.688679).  Saving model ...\n",
            "\tTrain_loss : 0.0656 Val_loss : 0.2524\n",
            "\tTrain_acc : 97.885% Val_acc : 92.689%\n"
          ]
        }
      ],
      "source": [
        "total_step = len(train_loader)\n",
        "total_step_val = len(valid_loader)\n",
        "\n",
        "early_stopping_patience = 4\n",
        "early_stopping_counter = 0\n",
        "\n",
        "valid_acc_max = 0 # Initialize best accuracy top 0\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "\n",
        "    #lists to host the train and validation losses of every batch for each epoch\n",
        "    train_loss, valid_loss  = [], []\n",
        "    #lists to host the train and validation accuracy of every batch for each epoch\n",
        "    train_acc, valid_acc  = [], []\n",
        "\n",
        "    #lists to host the train and validation predictions of every batch for each epoch\n",
        "    y_train_list, y_val_list = [], []\n",
        "\n",
        "    #initalize number of total and correctly classified texts during training and validation\n",
        "    correct, correct_val = 0, 0\n",
        "    total, total_val = 0, 0\n",
        "    running_loss, running_loss_val = 0, 0\n",
        "\n",
        "\n",
        "    ####TRAINING LOOP####\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE) #load features and targets in device\n",
        "\n",
        "        h = model.init_hidden(labels.size(0))\n",
        "\n",
        "        model.zero_grad() #reset gradients \n",
        "\n",
        "        output, h = model(inputs,h) #get output and hidden states from LSTM network\n",
        "        \n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        y_pred_train = torch.argmax(output, dim=1) #get tensor of predicted values on the training set\n",
        "        y_train_list.extend(y_pred_train.squeeze().tolist()) #transform tensor to list and the values to the list\n",
        "        \n",
        "        correct += torch.sum(y_pred_train==labels).item() #count correctly classified texts per batch\n",
        "        total += labels.size(0) #count total texts per batch\n",
        "\n",
        "    train_loss.append(running_loss / total_step)\n",
        "    train_acc.append(100 * correct / total)\n",
        "\n",
        "    ####VALIDATION LOOP####\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            val_h = model.init_hidden(labels.size(0))\n",
        "\n",
        "            output, val_h = model(inputs, val_h)\n",
        "\n",
        "            val_loss = criterion(output, labels)\n",
        "            running_loss_val += val_loss.item()\n",
        "\n",
        "            y_pred_val = torch.argmax(output, dim=1)\n",
        "            y_val_list.extend(y_pred_val.squeeze().tolist())\n",
        "\n",
        "            correct_val += torch.sum(y_pred_val==labels).item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "        valid_loss.append(running_loss_val / total_step_val)\n",
        "        valid_acc.append(100 * correct_val / total_val)\n",
        "\n",
        "    #Save model if validation accuracy increases\n",
        "    if np.mean(valid_acc) >= valid_acc_max:\n",
        "        torch.save(model.state_dict(), './state_dict.pt')\n",
        "        print(f'Epoch {e+1}:Validation accuracy increased ({valid_acc_max:.6f} --> {np.mean(valid_acc):.6f}).  Saving model ...')\n",
        "        valid_acc_max = np.mean(valid_acc)\n",
        "        early_stopping_counter=0 #reset counter if validation accuracy increases\n",
        "    else:\n",
        "        print(f'Epoch {e+1}:Validation accuracy did not increase')\n",
        "        early_stopping_counter+=1 #increase counter if validation accuracy does not increase\n",
        "        \n",
        "    if early_stopping_counter > early_stopping_patience:\n",
        "        print('Early stopped at epoch :', e+1)\n",
        "        break\n",
        "    \n",
        "    print(f'\\tTrain_loss : {np.mean(train_loss):.4f} Val_loss : {np.mean(valid_loss):.4f}')\n",
        "    print(f'\\tTrain_acc : {np.mean(train_acc):.3f}% Val_acc : {np.mean(valid_acc):.3f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfTLdAi45Qsd",
        "outputId": "ff267494-a15c-4f94-ef66-791582a5780b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading the best model\n",
        "model.load_state_dict(torch.load('./state_dict.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4w9sai14OH6"
      },
      "source": [
        "# LSTM Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ERsOjzW41tF"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "y_pred_list = []\n",
        "y_test_list = []\n",
        "for inputs, labels in test_loader:\n",
        "    inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "    test_h = model.init_hidden(labels.size(0))\n",
        "\n",
        "    output, val_h = model(inputs, test_h)\n",
        "    y_pred_test = torch.argmax(output, dim=1)\n",
        "    y_pred_list.extend(y_pred_test.squeeze().tolist())\n",
        "    y_test_list.extend(labels.squeeze().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "tu6ex4zC5Qse",
        "outputId": "d61a645f-c12f-4608-f6d0-40a7fce847c1"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-edbae1dfe284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Classification Report for Bi-LSTM :\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2133\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2135\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2136\u001b[0m             )\n\u001b[1;32m   2137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of classes, 3, does not match size of target_names, 5. Try specifying the labels parameter"
          ]
        }
      ],
      "source": [
        "print('Classification Report for Bi-LSTM :\\n', classification_report(y_test_list, y_pred_list, target_names=sentiments))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmE9K77s1qv_"
      },
      "outputs": [],
      "source": [
        "conf_matrix(y_test_list,y_pred_list,'PyTorch Bi-LSTM Sentiment Analysis\\nConfusion Matrix', sentiments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG9JGPla4OH7"
      },
      "source": [
        "**The performance scores of the algorithm are very high, with an overall accuracy of 94%.**<br>\n",
        "**In particular, the F1 scores for the more populated classes are over 95%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLM3-5xc4OH7"
      },
      "source": [
        "# BERT Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjBVnHdk4OH7"
      },
      "source": [
        "In this section, we will load a pre trained BERT model from the Hugging Face library and fine tune it for our classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWsonAbJ4OH7"
      },
      "source": [
        "First, we need to split the dataset into train - validation - test again since we need to tokenize the sentences differently from before (Naive Bayes and LSTM)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KFtmJsu4OH7"
      },
      "source": [
        "## Train - Validation - Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhOZDpg_4OH7"
      },
      "outputs": [],
      "source": [
        "X = df['text_clean'].values\n",
        "y = df['sentiment'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0oQ58BE4OH8"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT1XZtTY4OH8"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=seed_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teGFoANS4OH8"
      },
      "source": [
        "As seen before, we oversample the text to the majority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBdA4p8B4OH8"
      },
      "outputs": [],
      "source": [
        "ros = RandomOverSampler()\n",
        "X_train_os, y_train_os = ros.fit_resample(np.array(X_train).reshape(-1,1),np.array(y_train).reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4E_aUAe4OH8"
      },
      "outputs": [],
      "source": [
        "X_train_os = X_train_os.flatten()\n",
        "y_train_os = y_train_os.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU241FeU4OH8"
      },
      "outputs": [],
      "source": [
        "(unique, counts) = np.unique(y_train_os, return_counts=True)\n",
        "np.asarray((unique, counts)).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z66k9C034OH8"
      },
      "source": [
        "# BERT Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQoL1sZ74OH9"
      },
      "source": [
        "Since we need to tokenize the tweets (get \"input ids\" and \"attention masks\") for BERT, we load the specific BERT tokenizer from the Hugging Face library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWabG2M94OH9"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcC3KeUs4OH9"
      },
      "source": [
        "Then we define a custom tokenizer function using the loaded tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc0XnStr4OH9"
      },
      "outputs": [],
      "source": [
        "def bert_tokenizer(data):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in data:\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=sent,\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n",
        "            max_length=MAX_LEN,             # Choose max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length \n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yet0mwHv4OH9"
      },
      "source": [
        "Since we need to specify the length of the longest tokenized sentence, we tokenize the train tweets using the \"encode\" method of the original BERT tokenizer and check the longest sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kp5LPHP4OH9"
      },
      "outputs": [],
      "source": [
        "# Tokenize train tweets\n",
        "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train]\n",
        "\n",
        "# Find the longest tokenized tweet\n",
        "max_len = max([len(sent) for sent in encoded_tweets])\n",
        "print('Max length: ', max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0WPmQBu4OH9"
      },
      "source": [
        "We can choose the max length as 128."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY4lS6PA4OH9"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEW-zyh54OH-"
      },
      "source": [
        "Then we can tokenize the train, validation and test tweets using the custom define tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnkCSRVB4OH-"
      },
      "outputs": [],
      "source": [
        "train_inputs, train_masks = bert_tokenizer(X_train_os)\n",
        "val_inputs, val_masks = bert_tokenizer(X_valid)\n",
        "test_inputs, test_masks = bert_tokenizer(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEFCOhDN4OH-"
      },
      "source": [
        "## Data preprocessing for PyTorch BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4_q5Lf54OH-"
      },
      "source": [
        "Since we are using the BERT model built on PyTorch, we need to convert the arrays to pytorch tensors and create dataloaders for the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHfTF1PI4OH-"
      },
      "outputs": [],
      "source": [
        "# Convert target columns to pytorch tensors format\n",
        "train_labels = torch.from_numpy(y_train_os)\n",
        "val_labels = torch.from_numpy(y_valid)\n",
        "test_labels = torch.from_numpy(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvt7ohKD4OH-"
      },
      "source": [
        "## Dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxUXZnGt4OH-"
      },
      "source": [
        "To fine-tune the BERT model, the original authors recommend a batch size of 16 or 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-fcXCxP4OH-"
      },
      "outputs": [],
      "source": [
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7C7iMWi4OH-"
      },
      "outputs": [],
      "source": [
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWnd8w1e4OH_"
      },
      "source": [
        "# BERT Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCxTA6rs4OH_"
      },
      "source": [
        "Now we can create a custom BERT classifier class, including the original BERT model (made of transformer layers) and additional Dense layers to perform the desired classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9YPl4ha4OH_"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "class Bert_Classifier(nn.Module):\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        super(Bert_Classifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of the classifier, and number of labels\n",
        "        n_input = 768\n",
        "        n_hidden = 50\n",
        "        n_output = 5\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Add dense layers to perform the classification\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(n_input,  n_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hidden, n_output)\n",
        "        )\n",
        "        # Add possibility to freeze the BERT model\n",
        "        # to avoid fine tuning BERT params (usually leads to worse results)\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Feed input data to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tDTCWi64OH_"
      },
      "source": [
        "Moreover, since we want to define a learning rate scheduler, we define a custom \"initalize_model\" function as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-zggdxe4OH_"
      },
      "outputs": [],
      "source": [
        "def initialize_model(epochs=4):\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = Bert_Classifier(freeze_bert=False)\n",
        "    \n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Set up optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # learning rate, set to default value\n",
        "                      eps=1e-8    # decay, set to default value\n",
        "                      )\n",
        "    \n",
        "    ### Set up learning rate scheduler ###\n",
        "\n",
        "    # Calculate total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Defint the scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2rE8k4C4OH_"
      },
      "source": [
        "We also specify the use of GPU if present (highly recommended for the fine tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsYz1mhi4OIA"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "EPOCHS=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_WKXsYC4OIA"
      },
      "source": [
        "And then we intialize the BERT model calling the \"initialize_model\" function we defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQZVZvs74OIA"
      },
      "outputs": [],
      "source": [
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr21JZWi4OIA"
      },
      "source": [
        "# BERT Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqwQqKEg4OIA"
      },
      "source": [
        "After defining the custom BERT classifier model, we are ready to start the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id77eMyQ4OIA"
      },
      "outputs": [],
      "source": [
        "# Define Cross entropy Loss function for the multiclass classification task\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def bert_train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        print(\"-\"*10)\n",
        "        print(\"Epoch : {}\".format(epoch_i+1))\n",
        "        print(\"-\"*10)\n",
        "        print(\"-\"*38)\n",
        "        print(f\"{'BATCH NO.':^7} | {'TRAIN LOSS':^12} | {'ELAPSED (s)':^9}\")\n",
        "        print(\"-\"*38)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "        \n",
        "        ###TRAINING###\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            \n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass and get logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update model parameters:\n",
        "            # fine tune BERT params and train additional dense layers\n",
        "            optimizer.step()\n",
        "            # update learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 100 batches\n",
        "            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "                \n",
        "                print(f\"{step:^9} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        ###EVALUATION###\n",
        "        \n",
        "        # Put the model into the evaluation mode\n",
        "        model.eval()\n",
        "        \n",
        "        # Define empty lists to host accuracy and validation for each batch\n",
        "        val_accuracy = []\n",
        "        val_loss = []\n",
        "\n",
        "        for batch in val_dataloader:\n",
        "            batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\n",
        "            \n",
        "            # We do not want to update the params during the evaluation,\n",
        "            # So we specify that we dont want to compute the gradients of the tensors\n",
        "            # by calling the torch.no_grad() method\n",
        "            with torch.no_grad():\n",
        "                logits = model(batch_input_ids, batch_attention_mask)\n",
        "\n",
        "            loss = loss_fn(logits, batch_labels)\n",
        "\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "            # Get the predictions starting from the logits (get index of highest logit)\n",
        "            preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "            # Calculate the validation accuracy \n",
        "            accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
        "            val_accuracy.append(accuracy)\n",
        "\n",
        "        # Compute the average accuracy and loss over the validation set\n",
        "        val_loss = np.mean(val_loss)\n",
        "        val_accuracy = np.mean(val_accuracy)\n",
        "        \n",
        "        # Print performance over the entire training data\n",
        "        time_elapsed = time.time() - t0_epoch\n",
        "        print(\"-\"*61)\n",
        "        print(f\"{'AVG TRAIN LOSS':^12} | {'VAL LOSS':^10} | {'VAL ACCURACY (%)':^9} | {'ELAPSED (s)':^9}\")\n",
        "        print(\"-\"*61)\n",
        "        print(f\"{avg_train_loss:^14.6f} | {val_loss:^10.6f} | {val_accuracy:^17.2f} | {time_elapsed:^9.2f}\")\n",
        "        print(\"-\"*61)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nduKx7Tz4OIA",
        "outputId": "43701710-ba73-4023-be7b-a9fe2cb1475c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            "----------\n",
            "Epoch : 1\n",
            "----------\n",
            "--------------------------------------\n",
            "BATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n",
            "--------------------------------------\n",
            "   100    |   0.674154   |  3348.65 \n"
          ]
        }
      ],
      "source": [
        "bert_train(bert_classifier, train_dataloader, val_dataloader, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt4OEfQl4OIB"
      },
      "source": [
        "# BERT Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKUcrk4z4OIB"
      },
      "source": [
        "Now we define a function similar to the model \"evaluation\", where we feed to the model the test data instead of the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBQ3d04g4OIB"
      },
      "outputs": [],
      "source": [
        "def bert_predict(model, test_dataloader):\n",
        "    \n",
        "    # Define empty list to host the predictions\n",
        "    preds_list = []\n",
        "    \n",
        "    # Put the model into evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    for batch in test_dataloader:\n",
        "        batch_input_ids, batch_attention_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "        \n",
        "        # Avoid gradient calculation of tensors by using \"no_grad()\" method\n",
        "        with torch.no_grad():\n",
        "            logit = model(batch_input_ids, batch_attention_mask)\n",
        "        \n",
        "        # Get index of highest logit\n",
        "        pred = torch.argmax(logit,dim=1).cpu().numpy()\n",
        "        # Append predicted class to list\n",
        "        preds_list.extend(pred)\n",
        "\n",
        "    return preds_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBxu7O2t4OIB"
      },
      "source": [
        "Then we can call the defined function and get the class predictions of the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWD40mdz4OIB"
      },
      "outputs": [],
      "source": [
        "bert_preds = bert_predict(bert_classifier, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNegr4fU4OIB"
      },
      "outputs": [],
      "source": [
        "print('Classification Report for BERT :\\n', classification_report(y_test, bert_preds, target_names=sentiments))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pddq1No84OIB"
      },
      "outputs": [],
      "source": [
        "conf_matrix(y_test, bert_preds,' BERT Sentiment Analysis\\nConfusion Matrix', sentiments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg5-83x84OIC"
      },
      "source": [
        "**The performance scores of BERT Classifier are quite high and higher than those achieved using the LSTM model, with an overall accuracy around 95% and F1 scores well over 95%.**<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4CizAm24OIC"
      },
      "source": [
        "Thank your for checking out my notebook! Let me know if you have comments or if you want me to check out your work! :)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "u9wTHjTcc1D7",
        "28_q8-FP4OHp",
        "DGmYrF4d5QsP",
        "3aVlYw644OHq",
        "kx0IFdyO4OHv",
        "ELMGaiE-F7yn",
        "A0d0lSX0GbNS",
        "Geqa3e3zGUNL",
        "jCD4XitS4OHx",
        "A2mww5GA4OHz",
        "QZpta6PbBONf",
        "oTbzqr0u4OH1",
        "DWSPpkgh4OH2",
        "JD2NQRAU4OH4",
        "Sjv7lTKd4OH4",
        "5qQDp2AP4OH5",
        "t4w9sai14OH6",
        "mLM3-5xc4OH7",
        "5KFtmJsu4OH7",
        "Z66k9C034OH8",
        "ZEFCOhDN4OH-",
        "Pvt7ohKD4OH-",
        "LWnd8w1e4OH_",
        "Lr21JZWi4OIA",
        "Rt4OEfQl4OIB"
      ],
      "name": "MP.1_ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}